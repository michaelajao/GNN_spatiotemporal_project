{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# FILE: train_stan.ipynb\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from epiweeks import Week\n",
    "from utils import date_today, gravity_law_commute_dist  # Ensure these are defined in utils.py\n",
    "from model import STAN, CustomGraph  # Import STAN and CustomGraph from model.py\n",
    "\n",
    "# Ensure reproducibility\n",
    "RANDOM_SEED = 123\n",
    "def seed_torch(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '8'\n",
    "\n",
    "# Load and merge data\n",
    "raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n",
    "raw_data.to_csv('./data/state_covid_data.csv', index=False)\n",
    "pop_data = pd.read_csv('./data/uszips.csv')\n",
    "pop_data = pop_data.groupby('state_name').agg({\n",
    "    'population':'sum',\n",
    "    'density':'mean',\n",
    "    'lat':'mean',\n",
    "    'lng':'mean'\n",
    "}).reset_index()\n",
    "raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n",
    "\n",
    "# Generate location similarity based on Gravity Law\n",
    "loc_list = list(raw_data['state'].unique())\n",
    "loc_dist_map = {}\n",
    "\n",
    "for each_loc in loc_list:\n",
    "    loc_dist_map[each_loc] = {}\n",
    "    for each_loc2 in loc_list:\n",
    "        lat1 = raw_data[raw_data['state'] == each_loc]['lat'].unique()[0]\n",
    "        lng1 = raw_data[raw_data['state'] == each_loc]['lng'].unique()[0]\n",
    "        pop1 = raw_data[raw_data['state'] == each_loc]['population'].unique()[0]\n",
    "\n",
    "        lat2 = raw_data[raw_data['state'] == each_loc2]['lat'].unique()[0]\n",
    "        lng2 = raw_data[raw_data['state'] == each_loc2]['lng'].unique()[0]\n",
    "        pop2 = raw_data[raw_data['state'] == each_loc2]['population'].unique()[0]\n",
    "\n",
    "        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(\n",
    "            lat1, lng1, pop1,\n",
    "            lat2, lng2, pop2,\n",
    "            r=0.5\n",
    "        )\n",
    "\n",
    "num_locations = len(loc_list)\n",
    "print(f\"Number of unique locations: {num_locations}\")\n",
    "\n",
    "# Convert loc_dist_map to a DataFrame for visualization\n",
    "loc_dist_df = pd.DataFrame(loc_dist_map).fillna(0)\n",
    "\n",
    "# Create a heatmap of location similarities\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(loc_dist_df, cmap='viridis', linewidths=.5)\n",
    "plt.title('Location Similarity Based on Gravity Law')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Location')\n",
    "plt.show()\n",
    "\n",
    "# Generate Adjacency Map based on distance threshold\n",
    "dist_threshold = 18\n",
    "\n",
    "for each_loc in loc_dist_map:\n",
    "    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "adj_map = {}\n",
    "for each_loc in loc_dist_map:\n",
    "    adj_map[each_loc] = []\n",
    "    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n",
    "        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n",
    "            if i <= 3:\n",
    "                adj_map[each_loc].append(each_loc2)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            if i <= 1:\n",
    "                adj_map[each_loc].append(each_loc2)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "for each_loc in adj_map:\n",
    "    for each_loc2 in adj_map[each_loc]:\n",
    "        rows.append(loc_list.index(each_loc))\n",
    "        cols.append(loc_list.index(each_loc2))\n",
    "\n",
    "# Initialize adjacency matrix\n",
    "num_states = len(loc_list)\n",
    "adj_matrix = np.zeros((num_states, num_states), dtype=np.float32)\n",
    "\n",
    "# Create a mapping from state to index\n",
    "state_to_index = {state: idx for idx, state in enumerate(loc_list)}\n",
    "\n",
    "# Populate adjacency matrix based on adj_map\n",
    "for each_loc in adj_map:\n",
    "    i = state_to_index[each_loc]\n",
    "    for each_loc2 in adj_map[each_loc]:\n",
    "        j = state_to_index[each_loc2]\n",
    "        adj_matrix[i][j] = 1  # Binary adjacency; set to 1 if connected\n",
    "\n",
    "# Add self-loops to the adjacency matrix\n",
    "adj_matrix += np.eye(num_states, dtype=np.float32)\n",
    "\n",
    "# Normalize adjacency matrix\n",
    "adj_matrix = adj_matrix / adj_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Convert adjacency matrix to tensor\n",
    "adj_matrix = torch.tensor(adj_matrix, dtype=torch.float32).to(device)\n",
    "print(f\"Adjacency Matrix Shape: {adj_matrix.shape}\")\n",
    "\n",
    "# Create a directed graph using NetworkX\n",
    "G_nx = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "for state in adj_map.keys():\n",
    "    G_nx.add_node(state)\n",
    "\n",
    "# Add edges\n",
    "for state, neighbors in adj_map.items():\n",
    "    for neighbor in neighbors:\n",
    "        G_nx.add_edge(state, neighbor)\n",
    "\n",
    "# Plot the adjacency graph\n",
    "plt.figure(figsize=(15, 12))\n",
    "pos = nx.spring_layout(G_nx, seed=RANDOM_SEED)  # For consistent layout\n",
    "nx.draw(\n",
    "    G_nx,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_size=3000,\n",
    "    node_color='skyblue',\n",
    "    font_size=10,\n",
    "    font_weight='bold',\n",
    "    edge_color='gray'\n",
    ")\n",
    "plt.title('Adjacency Map of States Based on Gravity Law')\n",
    "plt.show()\n",
    "\n",
    "# Initialize CustomGraph\n",
    "custom_g = CustomGraph(G_nx, device)\n",
    "\n",
    "# Preprocess features\n",
    "active_cases = []\n",
    "confirmed_cases = []\n",
    "new_cases = []\n",
    "death_cases = []\n",
    "static_feat = []\n",
    "\n",
    "for each_loc in loc_list:\n",
    "    active = raw_data[raw_data['state'] == each_loc]['active'].values\n",
    "    confirmed = raw_data[raw_data['state'] == each_loc]['confirmed'].values\n",
    "    new = raw_data[raw_data['state'] == each_loc]['new_cases'].values\n",
    "    deaths = raw_data[raw_data['state'] == each_loc]['deaths'].values\n",
    "    static = raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']].values\n",
    "    active_cases.append(active)\n",
    "    confirmed_cases.append(confirmed)\n",
    "    new_cases.append(new)\n",
    "    death_cases.append(deaths)\n",
    "    static_feat.append(static)\n",
    "\n",
    "active_cases = np.array(active_cases)\n",
    "confirmed_cases = np.array(confirmed_cases)\n",
    "death_cases = np.array(death_cases)\n",
    "new_cases = np.array(new_cases)\n",
    "static_feat = np.array(static_feat)[:, 0, :]\n",
    "recovered_cases = confirmed_cases - active_cases - death_cases\n",
    "susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n",
    "\n",
    "# Compute differences for dynamic features\n",
    "dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases, axis=1)), axis=-1)\n",
    "dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases, axis=1)), axis=-1)\n",
    "dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases, axis=1)), axis=-1)\n",
    "\n",
    "# Build normalizer\n",
    "normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n",
    "\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n",
    "    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n",
    "    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n",
    "    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n",
    "    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n",
    "    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))\n",
    "\n",
    "# Prepare data for training, validation, and testing\n",
    "def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n",
    "    # Data shape: n_loc, timestep, n_feat\n",
    "    n_loc = data.shape[0]\n",
    "    timestep = data.shape[1]\n",
    "    n_feat = data.shape[2]\n",
    "\n",
    "    x = []\n",
    "    y_I = []\n",
    "    y_R = []\n",
    "    last_I = []\n",
    "    last_R = []\n",
    "    concat_I = []\n",
    "    concat_R = []\n",
    "    for i in range(0, timestep, slide_step):\n",
    "        if i + history_window + pred_window - 1 >= timestep or i + history_window >= timestep:\n",
    "            break\n",
    "        x.append(data[:, i:i + history_window, :].reshape((n_loc, history_window * n_feat)))\n",
    "\n",
    "        concat_I.append(data[:, i + history_window - 1, 0])\n",
    "        concat_R.append(data[:, i + history_window - 1, 1])\n",
    "        last_I.append(sum_I[:, i + history_window - 1])\n",
    "        last_R.append(sum_R[:, i + history_window - 1])\n",
    "\n",
    "        y_I.append(data[:, i + history_window:i + history_window + pred_window, 0])\n",
    "        y_R.append(data[:, i + history_window:i + history_window + pred_window, 1])\n",
    "\n",
    "    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n",
    "    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n",
    "    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n",
    "    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n",
    "    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n",
    "    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n",
    "    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n",
    "    return x, last_I, last_R, concat_I, concat_R, y_I, y_R\n",
    "\n",
    "valid_window = 25\n",
    "test_window = 25\n",
    "\n",
    "history_window = 6\n",
    "pred_window = 15\n",
    "slide_step = 5\n",
    "\n",
    "normalize = True\n",
    "\n",
    "dynamic_feat = np.concatenate(\n",
    "    (\n",
    "        np.expand_dims(dI, axis=-1),\n",
    "        np.expand_dims(dR, axis=-1),\n",
    "        np.expand_dims(dS, axis=-1)\n",
    "    ),\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "# Normalize dynamic features\n",
    "if normalize:\n",
    "    for i, each_loc in enumerate(loc_list):\n",
    "        dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n",
    "        dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n",
    "        dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n",
    "\n",
    "dI_mean = []\n",
    "dI_std = []\n",
    "dR_mean = []\n",
    "dR_std = []\n",
    "\n",
    "for i, each_loc in enumerate(loc_list):\n",
    "    dI_mean.append(normalizer['dI'][each_loc][0])\n",
    "    dR_mean.append(normalizer['dR'][each_loc][0])\n",
    "    dI_std.append(normalizer['dI'][each_loc][1])\n",
    "    dR_std.append(normalizer['dR'][each_loc][1])\n",
    "\n",
    "dI_mean = np.array(dI_mean)\n",
    "dI_std = np.array(dI_std)\n",
    "dR_mean = np.array(dR_mean)\n",
    "dR_std = np.array(dR_std)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_feat = dynamic_feat[:, :-valid_window - test_window, :]\n",
    "val_feat = dynamic_feat[:, -valid_window - test_window:-test_window, :]\n",
    "test_feat = dynamic_feat[:, -test_window:, :]\n",
    "\n",
    "train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR = prepare_data(\n",
    "    train_feat,\n",
    "    active_cases[:, :-valid_window - test_window],\n",
    "    recovered_cases[:, :-valid_window - test_window],\n",
    "    history_window,\n",
    "    pred_window,\n",
    "    slide_step\n",
    ")\n",
    "val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR = prepare_data(\n",
    "    val_feat,\n",
    "    active_cases[:, -valid_window - test_window:-test_window],\n",
    "    recovered_cases[:, -valid_window - test_window:-test_window],\n",
    "    history_window,\n",
    "    pred_window,\n",
    "    slide_step\n",
    ")\n",
    "test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR = prepare_data(\n",
    "    test_feat,\n",
    "    active_cases[:, -test_window:],\n",
    "    recovered_cases[:, -test_window:],\n",
    "    history_window,\n",
    "    pred_window,\n",
    "    slide_step\n",
    ")\n",
    "\n",
    "# Initialize the STAN model\n",
    "model = STAN(\n",
    "    custom_g,\n",
    "    in_dim=3 * history_window,\n",
    "    hidden_dim1=32,\n",
    "    hidden_dim2=32,\n",
    "    gru_dim=32,\n",
    "    num_heads=1,\n",
    "    pred_window=pred_window,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convert data to tensors and move to device\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32).to(device)\n",
    "train_I = torch.tensor(train_I, dtype=torch.float32).to(device)\n",
    "train_R = torch.tensor(train_R, dtype=torch.float32).to(device)\n",
    "train_cI = torch.tensor(train_cI, dtype=torch.float32).to(device)\n",
    "train_cR = torch.tensor(train_cR, dtype=torch.float32).to(device)\n",
    "train_yI = torch.tensor(train_yI, dtype=torch.float32).to(device)\n",
    "train_yR = torch.tensor(train_yR, dtype=torch.float32).to(device)\n",
    "\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32).to(device)\n",
    "val_I = torch.tensor(val_I, dtype=torch.float32).to(device)\n",
    "val_R = torch.tensor(val_R, dtype=torch.float32).to(device)\n",
    "val_cI = torch.tensor(val_cI, dtype=torch.float32).to(device)\n",
    "val_cR = torch.tensor(val_cR, dtype=torch.float32).to(device)\n",
    "val_yI = torch.tensor(val_yI, dtype=torch.float32).to(device)\n",
    "val_yR = torch.tensor(val_yR, dtype=torch.float32).to(device)\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32).to(device)\n",
    "test_I = torch.tensor(test_I, dtype=torch.float32).to(device)\n",
    "test_R = torch.tensor(test_R, dtype=torch.float32).to(device)\n",
    "test_cI = torch.tensor(test_cI, dtype=torch.float32).to(device)\n",
    "test_cR = torch.tensor(test_cR, dtype=torch.float32).to(device)\n",
    "test_yI = torch.tensor(test_yI, dtype=torch.float32).to(device)\n",
    "test_yR = torch.tensor(test_yR, dtype=torch.float32).to(device)\n",
    "\n",
    "dI_mean = torch.tensor(dI_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n",
    "dI_std = torch.tensor(dI_std, dtype=torch.float32).to(device).reshape((dI_std.shape[0], 1, 1))\n",
    "dR_mean = torch.tensor(dR_mean, dtype=torch.float32).to(device).reshape((dR_mean.shape[0], 1, 1))\n",
    "dR_std = torch.tensor(dR_std, dtype=torch.float32).to(device).reshape((dR_std.shape[0], 1, 1))\n",
    "\n",
    "N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)\n",
    "\n",
    "# Training parameters\n",
    "all_loss = []\n",
    "file_name = './save/stan.pth'\n",
    "min_loss = 1e10\n",
    "\n",
    "loc_name = 'California'\n",
    "cur_loc = loc_list.index(loc_name)\n",
    "\n",
    "epoch_count = 50 if normalize else 300\n",
    "scale = 0.1\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs('./save/', exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epoch_count), desc='Training Epochs'):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    try:\n",
    "        active_pred, recovered_pred, phy_active, phy_recover, _ = model(\n",
    "            train_x,\n",
    "            train_cI[cur_loc],\n",
    "            train_cR[cur_loc],\n",
    "            N[cur_loc],\n",
    "            train_I[cur_loc],\n",
    "            train_R[cur_loc],\n",
    "            torch.tensor(dI[cur_loc], dtype=torch.float32).to(device),\n",
    "            torch.tensor(dR[cur_loc], dtype=torch.float32).to(device),\n",
    "            h=None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during forward pass: {e}\")\n",
    "        break\n",
    "\n",
    "    # Normalize if required\n",
    "    if normalize:\n",
    "        phy_active = (phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n",
    "        phy_recover = (phy_recover - dR_mean[cur_loc]) / dR_std[cur_loc]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = (\n",
    "        criterion(active_pred.squeeze(), train_yI[cur_loc].squeeze()) +\n",
    "        criterion(recovered_pred.squeeze(), train_yR[cur_loc].squeeze()) +\n",
    "        scale * criterion(phy_active.squeeze(), train_yI[cur_loc].squeeze()) +\n",
    "        scale * criterion(phy_recover.squeeze(), train_yR[cur_loc].squeeze())\n",
    "    )\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_loss.append(loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Forward pass on training data to get hidden state\n",
    "            _, _, _, _, prev_h = model(\n",
    "                train_x,\n",
    "                train_cI[cur_loc],\n",
    "                train_cR[cur_loc],\n",
    "                N[cur_loc],\n",
    "                train_I[cur_loc],\n",
    "                train_R[cur_loc],\n",
    "                torch.tensor(dI[cur_loc], dtype=torch.float32).to(device),\n",
    "                torch.tensor(dR[cur_loc], dtype=torch.float32).to(device),\n",
    "                h=None\n",
    "            )\n",
    "            # Forward pass on validation data using previous hidden state\n",
    "            val_active_pred, val_recovered_pred, val_phy_active, val_phy_recover, _ = model(\n",
    "                val_x,\n",
    "                val_cI[cur_loc],\n",
    "                val_cR[cur_loc],\n",
    "                N[cur_loc],\n",
    "                val_I[cur_loc],\n",
    "                val_R[cur_loc],\n",
    "                torch.tensor(dI[cur_loc], dtype=torch.float32).to(device),\n",
    "                torch.tensor(dR[cur_loc], dtype=torch.float32).to(device),\n",
    "                h=prev_h\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during validation pass: {e}\")\n",
    "            break\n",
    "\n",
    "        if normalize:\n",
    "            val_phy_active = (val_phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n",
    "            val_phy_recover = (val_phy_recover - dR_mean[cur_loc]) / dR_std[cur_loc]\n",
    "\n",
    "        val_loss = (\n",
    "            criterion(val_active_pred.squeeze(), val_yI[cur_loc].squeeze()) +\n",
    "            criterion(val_recovered_pred.squeeze(), val_yR[cur_loc].squeeze()) +\n",
    "            scale * criterion(val_phy_active.squeeze(), val_yI[cur_loc].squeeze()) +\n",
    "            scale * criterion(val_phy_recover.squeeze(), val_yR[cur_loc].squeeze())\n",
    "        )\n",
    "\n",
    "    # Save the model if validation loss has decreased\n",
    "    if val_loss.item() < min_loss:\n",
    "        state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, file_name)\n",
    "        min_loss = val_loss.item()\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{epoch_count}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(all_loss, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model\n",
    "checkpoint = torch.load(file_name)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "# Make predictions with the test set\n",
    "prev_x = torch.cat((train_x, val_x), dim=1)\n",
    "prev_I = torch.cat((train_I, val_I), dim=1)\n",
    "prev_R = torch.cat((train_R, val_R), dim=1)\n",
    "prev_cI = torch.cat((train_cI, val_cI), dim=1)\n",
    "prev_cR = torch.cat((train_cR, val_cR), dim=1)\n",
    "prev_dI = torch.cat((torch.tensor(dI, dtype=torch.float32).to(device), torch.tensor(dI, dtype=torch.float32).to(device)), dim=1)\n",
    "prev_dR = torch.cat((torch.tensor(dR, dtype=torch.float32).to(device), torch.tensor(dR, dtype=torch.float32).to(device)), dim=1)\n",
    "\n",
    "# Forward pass using combined train and validation data to get hidden state\n",
    "prev_active_pred, _, prev_phyactive_pred, _, h = model(\n",
    "    prev_x,\n",
    "    train_cI[cur_loc],\n",
    "    train_cR[cur_loc],\n",
    "    N[cur_loc],\n",
    "    prev_I[cur_loc],\n",
    "    prev_R[cur_loc],\n",
    "    prev_dI[cur_loc],\n",
    "    prev_dR[cur_loc],\n",
    "    h=None\n",
    ")\n",
    "\n",
    "# Forward pass on test set using the hidden state\n",
    "test_pred_active, test_pred_recovered, test_pred_phy_active, test_pred_phy_recover, _ = model(\n",
    "    test_x,\n",
    "    test_cI[cur_loc],\n",
    "    test_cR[cur_loc],\n",
    "    N[cur_loc],\n",
    "    test_I[cur_loc],\n",
    "    test_R[cur_loc],\n",
    "    torch.tensor(dI[cur_loc], dtype=torch.float32).to(device),\n",
    "    torch.tensor(dR[cur_loc], dtype=torch.float32).to(device),\n",
    "    h\n",
    ")\n",
    "\n",
    "if normalize:\n",
    "    print(f'Estimated alpha in SIR model: {model.alpha_scaled.item():.4f}')\n",
    "    print(f'Estimated beta in SIR model: {model.beta_scaled.item():.4f}')\n",
    "\n",
    "# Cumulate predicted dI\n",
    "pred_I = []\n",
    "\n",
    "for i in range(test_pred_active.size(1)):\n",
    "    if normalize:\n",
    "        cur_pred = (test_pred_active[0, i, :].detach().cpu().numpy() * dI_std[cur_loc].reshape(1, 1).detach().cpu().numpy()) + dI_mean[cur_loc].reshape(1, 1).detach().cpu().numpy()\n",
    "    else:\n",
    "        cur_pred = test_pred_active[0, i, :].detach().cpu().numpy()\n",
    "    cur_pred = np.cumsum(cur_pred)\n",
    "    cur_pred = cur_pred + test_I[cur_loc, i].detach().cpu().item()\n",
    "    pred_I.append(cur_pred)\n",
    "pred_I = np.array(pred_I)\n",
    "\n",
    "# Function to get real y values\n",
    "def get_real_y(data, history_window=5, pred_window=15, slide_step=5):\n",
    "    # Data shape: n_loc, timestep, n_feat\n",
    "    n_loc = data.shape[0]\n",
    "    timestep = data.shape[1]\n",
    "\n",
    "    y = []\n",
    "    for i in range(0, timestep, slide_step):\n",
    "        if i + history_window + pred_window - 1 >= timestep or i + history_window >= timestep:\n",
    "            break\n",
    "        y.append(data[:, i + history_window:i + history_window + pred_window])\n",
    "    y = np.array(y, dtype=np.float32).transpose((1, 0, 2))\n",
    "    return y\n",
    "\n",
    "I_true = get_real_y(active_cases[:], history_window, pred_window, slide_step)\n",
    "\n",
    "# Plot predictions vs true values for Active Cases\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(I_true[cur_loc, -1, :], c='r', label='Ground truth')\n",
    "plt.plot(pred_I[-1, :], c='b', label='Prediction')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Active Cases')\n",
    "plt.title('Active Cases: True vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-demand-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
